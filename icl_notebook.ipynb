{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, '/scratch/gpfs/ls1546/icl_experiments/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import embedding\n",
    "import transformer\n",
    "import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #augment MNIST with multiple tasks\n",
    "# n_tasks = 1\n",
    "# #including normal MNIST with no random matrix as well\n",
    "# aug_train = dataset_utils.AugmentedData(train_data,num_tasks=n_tasks,include_identity=True)\n",
    "# aug_test = dataset_utils.AugmentedData(test_data,num_tasks=n_tasks)\n",
    "# aug_test.adopt_transforms(aug_train) #make sure train & test tasks are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flash attention\n",
      "Embedding Transformer with 816332 parameters, 210360 parameters in embedder & 604682 parameters in transformer\n"
     ]
    }
   ],
   "source": [
    "#instantiate model\n",
    "model = transformer.ImageICLTransformer(d_model=64,device='cuda',block_size=100)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1]),\n",
       " tensor([4, 0, 2, 9, 7, 8, 1, 5, 6, 3]),\n",
       " tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([0,1,0,1,0,1,0,1,0,1])\n",
    "perm = torch.randperm(10)\n",
    "perm_labels = labels[perm]\n",
    "labels, perm, perm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m images \u001b[38;5;241m=\u001b[39m aug_images\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m labels \u001b[38;5;241m=\u001b[39m aug_labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m B,T,C,W,H \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     34\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mreshape(T,B,C,W,H) \u001b[38;5;66;03m#hack for now -- make batch dimension into sequence\u001b[39;00m\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mreshape(T,B)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#train the model to do MNIST classification\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "epochs = 1\n",
    "log_freq = 100\n",
    "device = 'cuda'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50])\n",
    "\n",
    "loss_history = []\n",
    "seeds = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = [0]*(len(train_data)//batch_size)\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # apply transform\n",
    "        aug_images, aug_labels, seed = dataset_utils.get_transformed_batch(images, labels, seed=i)\n",
    "        seeds.append(seed)\n",
    "        optimizer.zero_grad()\n",
    "        images = aug_images.unsqueeze(1).unsqueeze(1)\n",
    "        labels = aug_labels.unsqueeze(1)\n",
    "        B,T,C,W,H = images.shape\n",
    "        images = images.reshape(T,B,C,W,H) #hack for now -- make batch dimension into sequence\n",
    "        labels = labels.reshape(T,B)\n",
    "        outputs = model((images,labels))\n",
    "        pred = outputs[:,-1,:]\n",
    "        loss = criterion(pred,labels[:,-1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss[i] = loss.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    loss_history += epoch_loss\n",
    "    epoch_loss = np.array(epoch_loss)\n",
    "    print(epoch_loss.mean())\n",
    "\n",
    "loss_history = np.array(loss_history)\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataset, batch_size=150, device='cuda'):\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for i,(images, labels) in tqdm(enumerate(loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.unsqueeze(1).unsqueeze(1)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        outputs = model((images,labels))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "    accuracy = 100 * (correct.item()) / len(dataset)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [00:03, 17.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(model, aug_test) #80% test accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
